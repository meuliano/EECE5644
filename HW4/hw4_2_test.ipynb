{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from generate_data import hw2q1 as generate_data\n",
    "import pandas as pd \n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import numpy as np \n",
    "from numpy.random import default_rng\n",
    "import matplotlib.pyplot as plt \n",
    "import math\n",
    "import random\n",
    "from scipy.stats import multivariate_normal\n",
    "import matplotlib.patches as mpatches\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.patches import Patch\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.losses import MeanSquaredError\n",
    "\n",
    "def cross_validation(train_data, k=10):\n",
    "    '''\n",
    "    Cross-validation to determine best number of perceptrons\n",
    "    for an MLPclassifier.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: pd.DataFrame\n",
    "        The data to use\n",
    "    k: int, optional\n",
    "        Number of folds; k-fold cross validation\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    P: int\n",
    "        Optimal number of perceptrons.\n",
    "    results: pandas.DataFrame\n",
    "        Dataframe showing the probability of error and number\n",
    "        of perceptrons.\n",
    "    '''\n",
    "    global PERCEPTRONS\n",
    "    X = np.array([train_data['x0'], train_data['x1']]).reshape((1000,2)) #.T.tolist()\n",
    "    #Y = np.asarray(train_data['y'], dtype=\"|S6\")\n",
    "    Y = train_data['y']\n",
    "    Ps = [x for x in np.arange(1, 11, 1)]\n",
    "    cv = KFold(n_splits=k)\n",
    "    results = pd.DataFrame()\n",
    "    for p in Ps:\n",
    "        PERCEPTRONS = p\n",
    "        #errors = []\n",
    "        #for fold in folds:\n",
    "        #model.fit(X_train,y_train, epochs = 100,verbose=0)  \n",
    "        model = KerasClassifier(build_fn=create_model, epochs=150, batch_size=10, verbose=0)\n",
    "        print(\"starting\")\n",
    "        scores = cross_val_score(estimator=model, X=X, y=Y, cv=cv, scoring='neg_mean_squared_error', error_score='raise')\n",
    "        print(\"done\")\n",
    "        d = {'Mean Squared Error': np.absolute(np.mean(scores)), 'Number of Perceptrons': p}\n",
    "        results = results.append(d, ignore_index=True)\n",
    "    return results\n",
    "\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=PERCEPTRONS, activation='softplus', input_dim=2))\n",
    "    model.add(Dense(units=1))\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mse'])\n",
    "    return model\n",
    "\n",
    "def plot_cross_validation_results(data):\n",
    "    '''\n",
    "    Plot how the hyperparameter changes the cross validation results.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    Data: pd.DataFrame\n",
    "        Data on the cross validation previously done.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    '''\n",
    "    fig, ax = plt.subplots(1,1,figsize=(5,2.8))\n",
    "    fig.subplots_adjust(left=0.11, right=0.985, top=0.98, bottom=0.16, wspace=0)\n",
    "    data['Training Dataset Size'] = data['Training Dataset Size'].astype(str)\n",
    "    sns.scatterplot(data=data, x='Number of Perceptrons', y='Mean Probability of Error Score', hue='Training Dataset Size', palette='ch:s=-.2,r=.6')\n",
    "    ax.set_ylim(0,1)\n",
    "    plt.savefig('./q1_cross_validation.pdf')\n",
    "    plt.clf()\n",
    "    plt.close()\n",
    "    return None\n",
    "\n",
    "def test_hyperparameters(train_data, test_data, p):\n",
    "    '''\n",
    "    Trains an MLP with the appropriate number of perceptrons, determined previously by\n",
    "    10-fold cross validation, using the entire respective training set.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_data: pd.DataFrame\n",
    "        Data to train the model\n",
    "    test_data: pd.DataFrame\n",
    "        Data to test the accuracy of the model\n",
    "    p: int\n",
    "        Number of perceptrons to use in the model.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    test_data: pd.DataFrame\n",
    "        Predicted values and original testing data.\n",
    "    '''\n",
    "    # Train the model using the optimized hyperparameters\n",
    "    X = np.array([train_data['x0'], train_data['x1']]).reshape((1000,2)) #.T.tolist()\n",
    "    Y = train_data['y']\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=p, activation='softplus', input_dim=2))\n",
    "    model.add(Dense(units=1))\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mse'])\n",
    "    model.fit(X, Y)\n",
    "    # Test the accuracy of the model\n",
    "    X = np.array([test_data['x0'], test_data['x1']]).reshape((10000,2)) #.T.tolist()\n",
    "    Y = model.predict(X)\n",
    "    test_data['Predicted Label'] = Y\n",
    "    return test_data\n",
    "\n",
    "def plot_model_results(data):\n",
    "    '''\n",
    "    Plot the accuracy of determined hyperparameters in training the model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    Data: pd.DataFrame\n",
    "        Data on the model fit previously done.\n",
    "    training_size: int\n",
    "        Number of sample in training dataset.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    '''\n",
    "    fig = plt.figure(figsize=(5,5))\n",
    "    fig.subplots_adjust(left=0.01, right=0.985, top=0.99, bottom=0.01, wspace=0)\n",
    "    ax = plt.axes(projection =\"3d\")\n",
    "    correct = 0\n",
    "    for idx,row in data.iterrows():\n",
    "        true_label = row['y']\n",
    "        decision   = row['Predicted Label']\n",
    "        x = row['x0']\n",
    "        y = row['x1']\n",
    "        ax.scatter3D(x,y,true_label, marker='.',color='g',alpha=0.1)\n",
    "        ax.scatter3D(x,y,decision, marker='.',color='b',alpha=0.1)\n",
    "    mse = MeanSquaredError()\n",
    "    accuracy = mse(data['y'], data['Predicted Label']).numpy()\n",
    "    print('Model accuracy was %.3f'%(accuracy))\n",
    "    legend_elements = [Patch(facecolor='g', edgecolor='g', label='Original'),\n",
    "                        Patch(facecolor='b', edgecolor='b', label='Predicted')]\n",
    "    ax.legend(handles=legend_elements, title='Value', loc='upper right')\n",
    "    ax.set_xlabel('x0')\n",
    "    ax.set_ylabel('x1')\n",
    "    ax.set_zlabel('y')\n",
    "    plt.savefig('./q1_classified_data.jpg')\n",
    "    plt.clf()\n",
    "    return accuracy\n",
    "\n",
    "if __name__=='__main__':\n",
    "    # train has 1000 samples, validate has 10000 samples\n",
    "    x_train, y_train, x_test, y_test = generate_data()\n",
    "    train = pd.DataFrame({'x0':x_train[0],'x1':x_train[1],'y':y_train})\n",
    "    test = pd.DataFrame({'x0':x_test[0],'x1':x_test[1],'y':y_test})\n",
    "\n",
    "    results = cross_validation(train, k=10)\n",
    "    results.to_csv('./q1_cv_results.csv')\n",
    "\n",
    "    df = pd.read_csv('./q1_cv_results.csv', index_col=0)\n",
    "    df = df.sort_values(by='Mean Squared Error').iloc[0]\n",
    "    p = df['Number of Perceptrons']\n",
    "    results = test_hyperparameters(train, test, p)\n",
    "    results.to_csv('q1_classified_results.csv')\n",
    "    plot_model_results(results)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
